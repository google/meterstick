{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 278,
          "status": "ok",
          "timestamp": 1758748301278,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "m5WwEa0hipA6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import attrs\n",
        "import copy\n",
        "from typing import Optional\n",
        "import pandas_gbq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vFnfrp-wSMB"
      },
      "source": [
        "## Set up data\n",
        "\n",
        "You will need a GCP project id. To get one,\n",
        "\n",
        "1.   Use the [Cloud Resource Manager](https://console.cloud.google.com/cloud-resource-manager) to Create a Cloud Platform project if you do not already have one.\n",
        "2.   [Enable billing](https://support.google.com/cloud/answer/6293499#enable-billing) for the project.\n",
        "3.   [Enable BigQuery](https://console.cloud.google.com/flows/enableapi?apiid=bigquery) APIs for the project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3805,
          "status": "ok",
          "timestamp": 1747687894704,
          "user": {
            "displayName": "xunmo yang",
            "userId": "12743905650398175098"
          },
          "user_tz": 420
        },
        "id": "cF0RCgRxirOS",
        "outputId": "4f5b7df4-60ba-41e3-bde9-fdfdabba4f7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00\u003c00:00, 9822.73it/s]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "n = 100\n",
        "df = pd.DataFrame({\n",
        "    'lost': np.random.choice([0, 1.], n),\n",
        "    'region': np.random.choice(('US', 'non-US'), n),\n",
        "    'experiment': np.random.choice(('control', 'experiment1', 'experiment2', 'experiment3'), n),\n",
        "})\n",
        "project_id='meterstick-personal'\n",
        "pandas_gbq.to_gbq(df, 'demo.data', project_id=project_id, if_exists='replace')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E-zgbv2wU8N"
      },
      "source": [
        "## Metric implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROesau-Sis_8"
      },
      "outputs": [],
      "source": [
        "class Metric:\n",
        "\n",
        "  def compute_on(self, data, split_by=None):\n",
        "    if split_by:\n",
        "      res = self.compute(data.groupby(split_by))\n",
        "    else:\n",
        "      res = [self.compute(data)]\n",
        "    res = pd.DataFrame(res)\n",
        "    res.columns = self.names\n",
        "    return res\n",
        "\n",
        "  def set_names(self, names):\n",
        "    self._names = names\n",
        "    return self\n",
        "\n",
        "  @property\n",
        "  def names(self):\n",
        "    return getattr(self, '_names', self.default_names)\n",
        "\n",
        "  def sql_aggregate(self, data, dimensions):\n",
        "    # Helper function for constructing aggregation queries.\n",
        "    dim_sql = ','.join(dimensions) + ',' if dimensions else ''\n",
        "    groupby = f'GROUP BY ' + ','.join(dimensions) if dim_sql else ''\n",
        "    val_cols = ','.join([f'{s} AS {n}' for s, n in zip(self.sql, self.names)])\n",
        "    return f'SELECT {dim_sql} {val_cols} FROM {data} {groupby}'\n",
        "\n",
        "  def to_sql(self, data, split_by=None):\n",
        "    return self.sql_aggregate(data, split_by)\n",
        "\n",
        "  def compute_on_sql(self, data, split_by=None):\n",
        "    res = pandas_gbq.read_gbq(self.to_sql(data, split_by), project_id=project_id)\n",
        "    dims = split_by + self.extra_dims\n",
        "    return res.set_index(dims).sort_index() if dims else res\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "    return Div(self, other)\n",
        "\n",
        "  def __or__(self, fn):\n",
        "    \"\"\"Overwrites the '|' operator to enable pipeline chaining.\"\"\"\n",
        "    return fn(self)\n",
        "\n",
        "\n",
        "class Operation(Metric):\n",
        "\n",
        "  def compute_on(self, data, split_by=None):\n",
        "    data_preprocessed = self.preprocess(data, split_by)\n",
        "    child_res = self.compute_children(data_preprocessed, split_by)\n",
        "    return self.process_results(child_res, split_by)\n",
        "\n",
        "  def __call__(self, child: Metric):\n",
        "    op = copy.deepcopy(self) if self.child else self\n",
        "    op.child = child\n",
        "    return op\n",
        "\n",
        "  def sql_select(self, data, dimensions):\n",
        "    # Helper function for constructing select queries.\n",
        "    dim_sql = ','.join(dimensions) + ',' if dimensions else ''\n",
        "    val_cols = ','.join([f'{s} AS {n}' for s, n in zip(self.sql, self.names)])\n",
        "    return f'SELECT {dim_sql} {val_cols} FROM {data}'\n",
        "\n",
        "  def to_sql(self, data, split_by=None):\n",
        "    data_preprocessed = self.preprocess_sql(data, split_by)\n",
        "    children_query = self.children_to_sql(data_preprocessed, split_by)\n",
        "    return self.assemble_query(children_query, split_by)\n",
        "\n",
        "  @property\n",
        "  def extra_dims(self):\n",
        "    return []\n",
        "\n",
        "\n",
        "@attrs.define\n",
        "class Sum(Metric):\n",
        "  var: str\n",
        "\n",
        "  def compute(self, data):\n",
        "    return data[self.var].sum()\n",
        "\n",
        "  @property\n",
        "  def default_names(self):\n",
        "    return [f'sum_{self.var}']\n",
        "\n",
        "  @property\n",
        "  def sql(self):\n",
        "    return [f'SUM({self.var})']\n",
        "\n",
        "\n",
        "@attrs.define\n",
        "class Count(Metric):\n",
        "  var: str\n",
        "\n",
        "  def compute(self, data):\n",
        "    return data[self.var].count()\n",
        "\n",
        "  @property\n",
        "  def default_names(self):\n",
        "    return [f'count_{self.var}']\n",
        "\n",
        "  @property\n",
        "  def sql(self):\n",
        "    return [f'COUNT({self.var})']\n",
        "\n",
        "\n",
        "@attrs.define\n",
        "class Div(Operation):\n",
        "  child1: Metric\n",
        "  child2: Metric\n",
        "\n",
        "  def preprocess(self, data, split_by):\n",
        "    return data\n",
        "\n",
        "  def compute_children(self, data, split_by):\n",
        "    return (self.child1.compute_on(data, split_by), self.child2.compute_on(data, split_by))\n",
        "\n",
        "  def process_results(self, child_res, split_by):\n",
        "    num, denom = child_res\n",
        "    num.columns = self.names\n",
        "    denom.columns = self.names\n",
        "    return num / denom\n",
        "\n",
        "  @property\n",
        "  def default_names(self):\n",
        "    return map('_div_'.join, zip(self.child1.names, self.child2.names))\n",
        "\n",
        "  def preprocess_sql(self, data, split_by):\n",
        "    return data\n",
        "\n",
        "  def children_to_sql(self, data, split_by):\n",
        "    return data\n",
        "\n",
        "  def assemble_query(self, child_res, split_by):\n",
        "    return self.sql_aggregate(child_res, split_by)\n",
        "\n",
        "  @property\n",
        "  def sql(self):\n",
        "    return map(' / '.join, zip(self.child1.sql, self.child2.sql))\n",
        "\n",
        "  @property\n",
        "  def extra_dims(self):\n",
        "    return []\n",
        "\n",
        "\n",
        "@attrs.define\n",
        "class PercentChange(Operation):\n",
        "  condition: str\n",
        "  baseline: str\n",
        "  child: Optional[Metric] = None\n",
        "\n",
        "  def preprocess(self, data, split_by):\n",
        "    return data\n",
        "\n",
        "  def compute_children(self, data, split_by):\n",
        "    return self.child.compute_on(data, split_by + [self.condition])\n",
        "\n",
        "  def process_results(self, child_res, split_by):\n",
        "    if split_by:\n",
        "      base = child_res.xs(self.baseline, level=self.condition)\n",
        "    else:\n",
        "      base = child_res.loc[self.baseline]\n",
        "    res = child_res / base - 1\n",
        "    res.columns = self.names\n",
        "    return res * 100\n",
        "\n",
        "  @property\n",
        "  def default_names(self):\n",
        "    return [f'pct_change_of_{n}' for n in self.child.names]\n",
        "\n",
        "  def preprocess_sql(self, data, split_by):\n",
        "    return data\n",
        "\n",
        "  def children_to_sql(self, data, split_by):\n",
        "    return self.child.to_sql(data, split_by + [self.condition])\n",
        "\n",
        "  def assemble_query(self, child_res, split_by):\n",
        "    dims = self.extra_dims + split_by\n",
        "    u = ','.join(dims[1:])\n",
        "    join = f'T JOIN Base USING ({u})'\n",
        "    if not u:\n",
        "      join = 'T CROSS JOIN Base'\n",
        "    return f\"\"\"\n",
        "    WITH T AS ({child_res}),\n",
        "    Base AS (SELECT *\n",
        "    EXCEPT ({self.condition}) FROM T\n",
        "    WHERE {self.condition}\n",
        "      = '{self.baseline}')\n",
        "    {self.sql_select(join, dims)}\"\"\"\n",
        "\n",
        "  @property\n",
        "  def sql(self):\n",
        "    return [f'(T.{c} / Base.{c} - 1) * 100' for c in self.child.names]\n",
        "\n",
        "  @property\n",
        "  def extra_dims(self):\n",
        "    return [self.condition] + self.child.extra_dims\n",
        "\n",
        "\n",
        "@attrs.define\n",
        "class Bootstrap(Operation):\n",
        "  n_rep: int = attrs.field(default=50)\n",
        "  child: Optional[Metric] = None\n",
        "\n",
        "  def preprocess(self, data, split_by):\n",
        "    for i in range(self.n_rep):\n",
        "      yield data.sample(frac=1, replace=True)\n",
        "\n",
        "  def compute_children(self, data, split_by):\n",
        "    sample_res = [self.child.compute_on(sample, split_by) for sample in data]\n",
        "    return pd.concat(sample_res, axis=1)\n",
        "\n",
        "  def process_results(self, child_res, split_by):\n",
        "    std = child_res.T.groupby(level=0).std().T\n",
        "    std.columns = self.names\n",
        "    return std\n",
        "\n",
        "  @property\n",
        "  def default_names(self):\n",
        "    return [f'se_{n}' for n in self.child.names]\n",
        "\n",
        "  def preprocess_sql(self, data, split_by):\n",
        "    return resample_n_times(data, split_by, self.n_rep)\n",
        "\n",
        "  def children_to_sql(self, data, split_by):\n",
        "    return (*data, self.child.to_sql('Samples', split_by + ['sample_idx']))\n",
        "\n",
        "  def assemble_query(self, child_res, split_by):\n",
        "    (input_data, samples, sample_res) = child_res\n",
        "    sql = self.sql_aggregate('SampleRes', split_by + self.extra_dims)\n",
        "    return f\"\"\"\n",
        "      CREATE TEMP TABLE Data\n",
        "        AS ({input_data});\n",
        "      WITH Samples AS ({samples}),\n",
        "      SampleRes AS ({sample_res})\n",
        "      {sql}\"\"\"\n",
        "\n",
        "  @property\n",
        "  def sql(self):\n",
        "    return [f'STDDEV({n})' for n in self.child.names]\n",
        "\n",
        "  @property\n",
        "  def extra_dims(self):\n",
        "    return self.child.extra_dims\n",
        "\n",
        "\n",
        "def resample_n_times(data, split_by, n_rep):\n",
        "  by_sql = ','.join(split_by) + ',' if split_by else ''\n",
        "  input_data = f\"\"\"\n",
        "    SELECT\n",
        "      *,\n",
        "      ROW_NUMBER() OVER (PARTITION BY sample_idx) AS row_number,\n",
        "      CEIL(RAND() * COUNT(*) OVER (PARTITION BY sample_idx))\n",
        "        AS random_row_number,\n",
        "    FROM {data},\n",
        "    UNNEST(GENERATE_ARRAY(1, {n_rep})) AS sample_idx\"\"\"\n",
        "  samples = f\"\"\"\n",
        "    SELECT b.*\n",
        "    FROM (\n",
        "      SELECT\n",
        "        {by_sql}\n",
        "        sample_idx,\n",
        "        random_row_number AS row_number\n",
        "      FROM Data) AS a\n",
        "    JOIN Data AS b\n",
        "    USING ({by_sql} sample_idx, row_number)\"\"\"\n",
        "  return (input_data, samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlpkx-U8wbHe"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5148,
          "status": "ok",
          "timestamp": 1747688722679,
          "user": {
            "displayName": "xunmo yang",
            "userId": "12743905650398175098"
          },
          "user_tz": 420
        },
        "id": "PYSSK5eEi0xI",
        "outputId": "13a70e02-93cf-4157-e3d0-176a28858a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "splitby is []\n",
            "\n",
            "\n",
            "Churn rate is    churn\n",
            "0   0.56\n",
            "\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Churn rate in SQL is    churn\n",
            "0   0.56\n",
            "\n",
            "Percent change is              pct_change_of_churn\n",
            "experiment                      \n",
            "control                 0.000000\n",
            "experiment1            -2.673797\n",
            "experiment2           -21.227621\n",
            "experiment3            -8.496732\n",
            "\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Percent change in SQL is              pct_change_of_churn\n",
            "experiment                      \n",
            "control                 0.000000\n",
            "experiment1            -2.673797\n",
            "experiment2           -21.227621\n",
            "experiment3            -8.496732\n",
            "\n",
            "Bootstrap is              se_pct_change_of_churn\n",
            "experiment                         \n",
            "control                    0.000000\n",
            "experiment1               26.014820\n",
            "experiment2               19.063005\n",
            "experiment3               18.845573\n",
            "\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Bootstrap in SQL is              se_pct_change_of_churn\n",
            "experiment                         \n",
            "control                    0.000000\n",
            "experiment1               24.857868\n",
            "experiment2               23.594318\n",
            "experiment3               22.693029\n",
            "\n"
          ]
        }
      ],
      "source": [
        "split_bys = [[], ]\n",
        "churn = (Sum(\"lost\") / Count(\"lost\")).set_names([\"churn\"])\n",
        "pct = churn | PercentChange(\"experiment\", \"control\")\n",
        "bst = pct | Bootstrap()\n",
        "\n",
        "for split_by in split_bys:\n",
        "  print(f'split_by is {split_by}\\n\\n')\n",
        "  print(f'Churn rate is {churn.compute_on(df, split_by)}\\n')\n",
        "  print(f'Churn rate in SQL is {churn.compute_on_sql(\"demo.data\", split_by)}\\n')\n",
        "\n",
        "  print(f'Percent change is {pct.compute_on(df, split_by)}\\n')\n",
        "  print(f'Percent change in SQL is {pct.compute_on_sql(\"demo.data\", split_by)}\\n')\n",
        "\n",
        "  print(f'Bootstrap is {bst.compute_on(df, split_by)}\\n')\n",
        "  print(f'Bootstrap in SQL is {bst.compute_on_sql(\"demo.data\", split_by)}\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//quality/ranklab/experimental/notebook:rl_colab",
        "kind": "private"
      },
      "provenance": [
        {
          "file_id": "1WgqbDALKD1FOG0HrdiuZSSrUhhtgEFyr",
          "timestamp": 1749145051098
        },
        {
          "file_id": "1wz69ygeXq1g8BtTR03Zmiv9H4gaYWW6C",
          "timestamp": 1749144947535
        },
        {
          "file_id": "1nJ-0cppF0C6rahGnQQITa7trVmNDhpFC",
          "timestamp": 1732592796289
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
